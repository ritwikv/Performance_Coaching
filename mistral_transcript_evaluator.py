"""
Mistral 7B Call Center Transcript Evaluator
Comprehensive evaluation system for call center performance coaching
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import json
import re
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Import required libraries
try:
    from llama_cpp import Llama
    print("✅ llama-cpp-python imported successfully")
except ImportError:
    print("❌ Please install llama-cpp-python: pip install llama-cpp-python")
    exit(1)

try:
    from ragas import evaluate
    from ragas.metrics import (
        context_precision,
        context_recall, 
        context_entity_recall,
        answer_relevancy,
        faithfulness
    )
    from datasets import Dataset
    print("✅ RAGAS imported successfully")
except ImportError:
    print("❌ Please install ragas: pip install ragas datasets")
    exit(1)

try:
    from textblob import TextBlob
    print("✅ TextBlob imported successfully")
except ImportError:
    print("❌ Please install textblob: pip install textblob")
    exit(1)

class MistralTranscriptEvaluator:
    """
    Comprehensive call center transcript evaluator using Mistral 7B
    """
    
    def __init__(self, model_path: str, n_ctx: int = 4096, n_threads: int = 4):
        """
        Initialize the Mistral evaluator
        
        Args:
            model_path (str): Path to the Mistral GGUF model file
            n_ctx (int): Context window size
            n_threads (int): Number of threads for inference
        """
        self.model_path = model_path
        self.n_ctx = n_ctx
        self.n_threads = n_threads
        
        print(f"🔄 Loading Mistral 7B model from: {model_path}")
        try:
            self.llm = Llama(
                model_path=model_path,
                n_ctx=n_ctx,
                n_threads=n_threads,
                verbose=False
            )
            print("✅ Mistral 7B model loaded successfully!")
        except Exception as e:
            print(f"❌ Error loading model: {e}")
            raise
        
        # Initialize knowledge base
        self.knowledge_documents = []
        self.crutch_words = [
            'um', 'uh', 'like', 'you know', 'actually', 'basically', 'literally',
            'obviously', 'definitely', 'absolutely', 'totally', 'really', 'very',
            'quite', 'pretty', 'sort of', 'kind of', 'I mean', 'well'
        ]
    
    def generate_response(self, prompt: str, max_tokens: int = 512) -> str:
        """
        Generate response from Mistral model
        
        Args:
            prompt (str): Input prompt
            max_tokens (int): Maximum tokens to generate
            
        Returns:
            str: Generated response
        """
        try:
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=0.3,
                top_p=0.9,
                echo=False,
                stop=["</s>", "[INST]", "[/INST]"]
            )
            return response['choices'][0]['text'].strip()
        except Exception as e:
            print(f"❌ Error generating response: {e}")
            return ""
    
    def evaluate_english_correctness(self, text: str) -> Dict[str, Any]:
        """
        Evaluate English language correctness and provide coaching
        
        Args:
            text (str): Text to evaluate
            
        Returns:
            Dict: Evaluation results and coaching feedback
        """
        prompt = f"""[INST] You are an expert English language coach for call center agents. 
        Analyze the following text for English language correctness and provide specific coaching feedback.

        Text to analyze: "{text}"

        Please provide:
        1. Grammar errors (if any) with corrections
        2. Spelling mistakes (if any) with corrections  
        3. Sentence structure improvements
        4. Professional language suggestions
        5. Overall English proficiency score (1-10)
        6. Specific coaching recommendations

        Format your response as structured feedback. [/INST]"""
        
        response = self.generate_response(prompt, max_tokens=400)
        
        # Extract score using regex
        score_match = re.search(r'score.*?(\d+(?:\.\d+)?)', response.lower())
        score = float(score_match.group(1)) if score_match else 7.0
        
        return {
            'english_score': score,
            'feedback': response,
            'has_errors': score < 8.0
        }
    
    def analyze_sentence_structure(self, text: str) -> Dict[str, Any]:
        """
        Analyze sentence length and recommend crisp sentences
        
        Args:
            text (str): Text to analyze
            
        Returns:
            Dict: Sentence analysis results
        """
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        sentence_lengths = [len(s.split()) for s in sentences]
        avg_length = np.mean(sentence_lengths) if sentence_lengths else 0
        long_sentences = [s for s in sentences if len(s.split()) > 20]
        
        prompt = f"""[INST] You are a communication coach. Analyze this text for sentence structure and clarity.

        Text: "{text}"
        
        Average sentence length: {avg_length:.1f} words
        Number of long sentences (>20 words): {len(long_sentences)}
        
        Please provide:
        1. Assessment of sentence clarity and conciseness
        2. Specific recommendations for making sentences more crisp
        3. Rewrite examples for any overly long sentences
        4. Communication effectiveness score (1-10)
        
        Focus on professional call center communication standards. [/INST]"""
        
        response = self.generate_response(prompt, max_tokens=400)
        
        # Extract score
        score_match = re.search(r'score.*?(\d+(?:\.\d+)?)', response.lower())
        clarity_score = float(score_match.group(1)) if score_match else 7.0
        
        return {
            'avg_sentence_length': avg_length,
            'long_sentences_count': len(long_sentences),
            'long_sentences': long_sentences,
            'clarity_score': clarity_score,
            'feedback': response
        }
    
    def detect_repetition_and_crutch_words(self, text: str) -> Dict[str, Any]:
        """
        Detect word repetition and crutch words
        
        Args:
            text (str): Text to analyze
            
        Returns:
            Dict: Repetition and crutch word analysis
        """
        words = re.findall(r'\b\w+\b', text.lower())
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        # Find repeated words (appearing more than expected frequency)
        repeated_words = {word: count for word, count in word_counts.items() 
                         if count > 2 and len(word) > 3}
        
        # Find crutch words
        found_crutch_words = {}
        for crutch in self.crutch_words:
            crutch_lower = crutch.lower()
            if crutch_lower in text.lower():
                count = text.lower().count(crutch_lower)
                if count > 0:
                    found_crutch_words[crutch] = count
        
        prompt = f"""[INST] You are a speech coach analyzing word usage patterns.

        Text: "{text}"
        
        Repeated words found: {repeated_words}
        Crutch words found: {found_crutch_words}
        
        Please provide:
        1. Assessment of word repetition issues
        2. Impact of crutch words on professionalism
        3. Specific coaching to reduce repetitive language
        4. Alternative vocabulary suggestions
        5. Speech clarity score (1-10)
        
        Focus on professional call center communication. [/INST]"""
        
        response = self.generate_response(prompt, max_tokens=400)
        
        # Extract score
        score_match = re.search(r'score.*?(\d+(?:\.\d+)?)', response.lower())
        speech_score = float(score_match.group(1)) if score_match else 7.0
        
        return {
            'repeated_words': repeated_words,
            'crutch_words': found_crutch_words,
            'speech_score': speech_score,
            'feedback': response
        }
    
    def create_knowledge_documents(self, df: pd.DataFrame) -> List[str]:
        """
        Create knowledge documents from transcripts
        
        Args:
            df (pd.DataFrame): DataFrame with call transcripts
            
        Returns:
            List[str]: Knowledge documents
        """
        print("🔄 Creating knowledge documents from transcripts...")
        
        # Combine all answers to create comprehensive knowledge base
        all_answers = df['Answers'].dropna().tolist()
        
        # Group answers by themes/topics
        prompt = f"""[INST] You are a knowledge management expert. Create comprehensive knowledge documents from these call center responses.

        Call center responses: {all_answers[:10]}  # Limit for context
        
        Please create structured knowledge documents covering:
        1. Common customer issues and solutions
        2. Standard procedures and policies
        3. Product/service information
        4. Best practices for customer service
        5. Troubleshooting guides
        
        Format as clear, factual knowledge articles that can be used as reference material. [/INST]"""
        
        knowledge_response = self.generate_response(prompt, max_tokens=800)
        
        # Split into individual knowledge documents
        knowledge_docs = re.split(r'\n\s*\d+\.|\n\s*-', knowledge_response)
        knowledge_docs = [doc.strip() for doc in knowledge_docs if doc.strip() and len(doc.strip()) > 50]
        
        self.knowledge_documents = knowledge_docs
        print(f"✅ Created {len(knowledge_docs)} knowledge documents")
        
        return knowledge_docs
    
    def evaluate_with_ragas(self, question: str, answer: str, contexts: List[str]) -> Dict[str, float]:
        """
        Evaluate using RAGAS metrics
        
        Args:
            question (str): Customer question
            answer (str): Agent answer
            contexts (List[str]): Knowledge documents as context
            
        Returns:
            Dict[str, float]: RAGAS metric scores
        """
        try:
            # Prepare dataset for RAGAS
            data = {
                'question': [question],
                'answer': [answer],
                'contexts': [contexts],
                'ground_truth': [answer]  # Using answer as ground truth for this use case
            }
            
            dataset = Dataset.from_dict(data)
            
            # Define metrics
            metrics = [
                context_precision,
                context_recall,
                context_entity_recall,
                answer_relevancy,
                faithfulness
            ]
            
            # Evaluate
            result = evaluate(dataset, metrics=metrics)
            
            return {
                'context_precision': result['context_precision'],
                'context_recall': result['context_recall'], 
                'context_entity_recall': result['context_entity_recall'],
                'answer_relevancy': result['answer_relevancy'],
                'faithfulness': result['faithfulness']
            }
            
        except Exception as e:
            print(f"⚠️ RAGAS evaluation error: {e}")
            # Return default scores if RAGAS fails
            return {
                'context_precision': 0.7,
                'context_recall': 0.7,
                'context_entity_recall': 0.7,
                'answer_relevancy': 0.7,
                'faithfulness': 0.7
            }
    
    def generate_ragas_coaching(self, scores: Dict[str, float], answer: str) -> str:
        """
        Generate coaching feedback based on RAGAS scores
        
        Args:
            scores (Dict[str, float]): RAGAS metric scores
            answer (str): Agent answer
            
        Returns:
            str: Coaching feedback
        """
        prompt = f"""[INST] You are a performance coach analyzing call center agent responses using quality metrics.

        Agent Response: "{answer}"
        
        Quality Metrics:
        - Context Precision: {scores['context_precision']:.2f}
        - Context Recall: {scores['context_recall']:.2f}
        - Context Entity Recall: {scores['context_entity_recall']:.2f}
        - Answer Relevancy: {scores['answer_relevancy']:.2f}
        - Faithfulness: {scores['faithfulness']:.2f}
        
        Please provide personalized coaching feedback in a conversational tone:
        1. Explain what each score means in simple terms
        2. Highlight strengths and areas for improvement
        3. Provide specific actionable recommendations
        4. Mention topics to brush up on if needed
        
        Format as encouraging, constructive coaching feedback. [/INST]"""
        
        return self.generate_response(prompt, max_tokens=400)
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        Analyze sentiment and provide feedback
        
        Args:
            text (str): Text to analyze
            
        Returns:
            Dict: Sentiment analysis results
        """
        # Use TextBlob for basic sentiment
        blob = TextBlob(text)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        
        # Use Mistral for detailed sentiment analysis
        prompt = f"""[INST] You are an emotional intelligence coach analyzing the sentiment and tone of this call center response.

        Response: "{text}"
        
        Please provide:
        1. Overall sentiment (positive, negative, neutral)
        2. Emotional tone assessment
        3. Professionalism level
        4. Customer impact analysis
        5. Coaching feedback on emotional communication
        
        Format as encouraging feedback for the agent. [/INST]"""
        
        sentiment_feedback = self.generate_response(prompt, max_tokens=300)
        
        # Determine sentiment category
        if polarity > 0.1:
            sentiment_category = "positive"
        elif polarity < -0.1:
            sentiment_category = "negative"
        else:
            sentiment_category = "neutral"
        
        return {
            'polarity': polarity,
            'subjectivity': subjectivity,
            'sentiment_category': sentiment_category,
            'feedback': sentiment_feedback
        }
    
    def summarize_question_topic(self, question: str) -> str:
        """
        Summarize the topic/theme of the customer question
        
        Args:
            question (str): Customer question
            
        Returns:
            str: Topic summary
        """
        prompt = f"""[INST] You are an expert at categorizing customer service inquiries. 

        Customer Question: "{question}"
        
        Please provide:
        1. Main topic/theme (1-2 words)
        2. Issue category (e.g., billing, technical, complaint, inquiry)
        3. Brief summary of what the customer needs
        4. Urgency level (low, medium, high)
        
        Format as a concise topic summary. [/INST]"""
        
        return self.generate_response(prompt, max_tokens=200)
    
    def evaluate_single_interaction(self, question: str, answer: str, interaction_id: int) -> Dict[str, Any]:
        """
        Comprehensive evaluation of a single question-answer interaction
        
        Args:
            question (str): Customer question
            answer (str): Agent answer
            interaction_id (int): Interaction identifier
            
        Returns:
            Dict: Complete evaluation results
        """
        print(f"🔄 Evaluating interaction {interaction_id}...")
        
        results = {
            'interaction_id': interaction_id,
            'question': question,
            'answer': answer,
            'timestamp': datetime.now().isoformat()
        }
        
        if pd.isna(answer) or not answer.strip():
            results['evaluation_status'] = 'skipped_no_answer'
            return results
        
        try:
            # 1. English correctness evaluation
            english_eval = self.evaluate_english_correctness(answer)
            results['english_evaluation'] = english_eval
            
            # 2. Sentence structure analysis
            sentence_eval = self.analyze_sentence_structure(answer)
            results['sentence_analysis'] = sentence_eval
            
            # 3. Repetition and crutch words
            repetition_eval = self.detect_repetition_and_crutch_words(answer)
            results['repetition_analysis'] = repetition_eval
            
            # 4. RAGAS evaluation (if knowledge documents exist)
            if self.knowledge_documents and not pd.isna(question):
                ragas_scores = self.evaluate_with_ragas(question, answer, self.knowledge_documents)
                results['ragas_scores'] = ragas_scores
                results['ragas_coaching'] = self.generate_ragas_coaching(ragas_scores, answer)
            
            # 5. Sentiment analysis
            sentiment_eval = self.analyze_sentiment(answer)
            results['sentiment_analysis'] = sentiment_eval
            
            # 6. Question topic summary (if question exists)
            if not pd.isna(question) and question.strip():
                topic_summary = self.summarize_question_topic(question)
                results['question_topic'] = topic_summary
            
            results['evaluation_status'] = 'completed'
            
        except Exception as e:
            print(f"⚠️ Error evaluating interaction {interaction_id}: {e}")
            results['evaluation_status'] = 'error'
            results['error_message'] = str(e)
        
        return results
    
    def evaluate_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Evaluate entire DataFrame of call transcripts
        
        Args:
            df (pd.DataFrame): DataFrame with Questions and Answers
            
        Returns:
            pd.DataFrame: DataFrame with evaluation results
        """
        print("🚀 Starting comprehensive transcript evaluation...")
        print(f"📊 Processing {len(df)} interactions...")
        
        # Create knowledge documents first
        self.create_knowledge_documents(df)
        
        # Evaluate each interaction
        evaluation_results = []
        
        for idx, row in df.iterrows():
            question = row.get('Questions', '')
            answer = row.get('Answers', '')
            interaction_id = row.get('interaction_id', idx + 1)
            
            result = self.evaluate_single_interaction(question, answer, interaction_id)
            evaluation_results.append(result)
        
        # Create results DataFrame
        results_df = pd.DataFrame(evaluation_results)
        
        print("✅ Evaluation completed!")
        return results_df
    
    def generate_summary_report(self, results_df: pd.DataFrame) -> str:
        """
        Generate a comprehensive summary report
        
        Args:
            results_df (pd.DataFrame): Evaluation results
            
        Returns:
            str: Summary report
        """
        completed_evals = results_df[results_df['evaluation_status'] == 'completed']
        
        if len(completed_evals) == 0:
            return "No completed evaluations to summarize."
        
        # Calculate average scores
        avg_english = np.mean([eval_data.get('english_score', 0) 
                              for eval_data in completed_evals['english_evaluation'] 
                              if eval_data])
        
        avg_clarity = np.mean([eval_data.get('clarity_score', 0) 
                              for eval_data in completed_evals['sentence_analysis'] 
                              if eval_data])
        
        avg_speech = np.mean([eval_data.get('speech_score', 0) 
                             for eval_data in completed_evals['repetition_analysis'] 
                             if eval_data])
        
        # Generate summary using Mistral
        prompt = f"""[INST] You are a call center performance analyst creating a summary report.

        Evaluation Summary:
        - Total interactions evaluated: {len(completed_evals)}
        - Average English proficiency score: {avg_english:.2f}/10
        - Average communication clarity score: {avg_clarity:.2f}/10  
        - Average speech quality score: {avg_speech:.2f}/10
        
        Please create a comprehensive performance summary report including:
        1. Overall performance assessment
        2. Key strengths identified
        3. Main areas for improvement
        4. Specific training recommendations
        5. Action plan for coaching
        
        Format as a professional coaching report. [/INST]"""
        
        return self.generate_response(prompt, max_tokens=600)

def main():
    """
    Main function to demonstrate the evaluator
    """
    # Configuration
    MODEL_PATH = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"  # Update this path
    
    print("🎯 Mistral 7B Call Center Transcript Evaluator")
    print("=" * 60)
    
    try:
        # Initialize evaluator
        evaluator = MistralTranscriptEvaluator(MODEL_PATH)
        
        # Load transcript data
        print("📂 Loading transcript data...")
        try:
            from extract_call_data_dataframe import extract_call_transcript_to_dataframe
            df = extract_call_transcript_to_dataframe("Call Transcript Sample 1.json")
            
            if df is None:
                print("❌ Failed to load transcript data")
                return
                
            print(f"✅ Loaded {len(df)} interactions")
            
        except ImportError:
            print("❌ Please ensure extract_call_data_dataframe.py is available")
            return
        
        # Evaluate transcripts
        results_df = evaluator.evaluate_dataframe(df)
        
        # Save results
        results_df.to_csv('transcript_evaluation_results.csv', index=False)
        results_df.to_json('transcript_evaluation_results.json', orient='records', indent=2)
        
        # Generate summary report
        summary_report = evaluator.generate_summary_report(results_df)
        
        with open('evaluation_summary_report.txt', 'w') as f:
            f.write(summary_report)
        
        print("\n" + "=" * 60)
        print("📊 EVALUATION SUMMARY")
        print("=" * 60)
        print(summary_report)
        
        print("\n" + "=" * 60)
        print("💾 FILES GENERATED:")
        print("✅ transcript_evaluation_results.csv - Detailed results")
        print("✅ transcript_evaluation_results.json - JSON format results")
        print("✅ evaluation_summary_report.txt - Summary report")
        
    except Exception as e:
        print(f"❌ Error: {e}")
        print("Please ensure:")
        print("1. Mistral model file path is correct")
        print("2. All required packages are installed")
        print("3. Transcript data file exists")

if __name__ == "__main__":
    main()

